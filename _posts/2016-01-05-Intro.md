---
layout: post
title: Introduction to Spark and the bus timetable data case study (DRAFT)
summary: This post describes what Spark is and why one might use it. It also describes the case study where Spark is applied.
---

## Big Data
Big data is a buzz-word that can be used to sell expensive services or wrap old concepts in a new, sexier, package.
At the same time, even after all the hype has been stripped off, there is no denying the paradigm shift that has been creeping to almost all areas of data analysis with an inevitable force during the past decade.
So how should big data be understood?

According to [Wikipedia](https://en.wikipedia.org/wiki/Big_data):

> Big data is a broad term for data sets so large or complex that traditional data processing applications are inadequate.

Size and complexity can be very subjective and indeed they can depend on the situation.
There have been arguments against using big data tools, probably caused by over-enthusiasm by
some and aggressive marketing by the vendors. The criticism often goes along the lines that
“your data is not so big that it could not be processed on a fast PC, with a large hard drive
using tool X and language Y”. One factor may also be the perceived excessive complexity and
expense of using traditional big data tools, many of which have been inspired and even developed
by the likes Google, Yahoo and Microsoft. Few use cases achieve quite the same scale.
So when should one consider using big data tools? Here are a few reasons for doing so:

* There simply is too much data to fit one the “one PC”. Since there are no feasible alternatives,
this is a no-brainer.
* Even though the above situation may not be the case today, there is clear potential that this may
change in the future. Investing time and effort into data analysis algorithm design and implementation
is unwise if there is no clear path for scalability for future needs.
* Data is stored in the cloud. Even though traditional data analysis tools can be run in the cloud,
most cloud service providers offer big data tools without the complexity and cost of owning the hardware.
* Data is unstructured. Many databases are great at storing and processing unstructured data, but
this is still an area where big data tools excel.
* Data needs to be secured. Backing up large data sets is a big pain. Distributed file systems and
databases offer a great solution for storing data with redundancy.

## Case study: Bus timetable accuracy
I am going to make a case study on analyzing public transportation timetable accuracy in Tampere,
Finland, where I live. All the buses in the town of Tampere are equipped with GPS and share their
[location](http://data.itsfactory.fi/siriaccess/vm/json). The travelers typically view the locations
like this.

<div  class="center"><a href="http://lissu.tampere.fi/"><img src="/BigDataSpark/assets/2016-01-05-Intro-Buses.png"/></a></div>

In addition, I’m using the local weather provided by the [Finnish Meteorological Institute](http://en.ilmatieteenlaitos.fi/open-data-manual).

My objectives for this case study are as follows:

* Calculate the travel time for specific bus routes and lines. I’ll be using Spark for this.
* Correlate the travel time with a few key parameters:
weekday, hour of the day, whether it’s a working day or not, temperature, rain.
I’ll be using [R](https://www.r-project.org/) for this.

## Choosing the big data tool
Having decided that using the big data approach is a viable option in my case, I need to choose from
the vast array of great tools available to prospective data scientists and engineers.
The [Apache Hadoop](https://hadoop.apache.org/) is probably the best known of the open source big data projects.
It includes modules such as the HDFS distributed file system, the YARN resource negotiator and the MapReduce
parallel processing framework. Well-known related projects are the Cassandra and HBase databases.
Hadoop MapReduce is one of the best known big data processing engines. However, it can be slow and
its programming model is often seen as cumbersome and restrictive. Typically, a truly massive amount of data
needs to be available for processing to justify using MapReduce. Apache Spark is a very popular and
modern engine for processing big data. It provides a nimbler alternative to MapReduce that is more flexible
to program and is often much faster. One great thing about Spark is that it belongs to the Hadoop ecosystem
and enjoys all the same benefits and interoperability with other Hadoop-compatible tools.

### Spark summarized
According to its web site,

> Apache Spark™ is a fast and general engine for large-scale data processing.

Here is a summary of the key benefits of Spark:

* Speed. Spark is optimized for in-memory parallel processing.
* Scalability, runs everywhere. Spark runs fine on a laptop, but scales, with relative ease,
to large clusters, and to the cloud. All the major cloud providers (e.g., Amazon, Google, Microsoft Azure, IBM) support
Spark as a service.
* Part of the established Hadoop ecosystem.
* Flexible, functional programming model. Spark programs can be written in Scala
(Spark’s native language), Python and Java. Simple Spark programs are literally like simple command line
utility scripts with very little overhead.
* Generality. Spark can be used to combine SQL, streaming, and complex analytics.

Spark applications running on a cluster consist of a driver program and multiple task executors
running on worker nodes. A cluster manager (Spark comes with its own or can use an external manager, such as Hadoop YARN)
is responsible for resource allocation across the cluster. On one hand, this can be completely transparent to the application programmer, which “lowers the bar”, but on the other hand, understanding the execution model is critical for writing high-performance parallel applications.


<div class="center">
	<a href="http://lissu.tampere.fi/"><img src="/BigDataSpark/assets/2016-01-05-Intro-Spark.png"/></a><br>
	Spark cluster mode components (from <a href="http://spark.apache.org/docs/latest/cluster-overview.html">Spark documentation</a>)
</div>

Spark also comes with a “REPL” called Spark shell. It’s a great tool for experimentation.

### Spark key concepts

* **Driver program** is responsible for loading Spark Context, which provides utility functions for data I/O.
The driver contains a main function and gets its parameters via command line arguments.
* **Resilient Distributed Data sets (RDDs)** An RDD is an immutable fault-tolerant collection of elements that can be
operated on in parallel. RDDs can be created either from existing in-memory collections or by loading data from an external
source such as the local file system, Hadoop HDFS, Amazon S3, or from any of the most popular databases. Each RDD is split
into multiple partitions that can be distributed to the cluster for computation.
* **Transformations** are operations (map, flatMap, reduce, fold, ..) on RDDs that create other RDDs. Transformations are lazily evaluated.
* **Actions** compute the result from the RDDs and the transformations on them, and either return the result to the driver
or save it to the file system, database etc.
* **RDD persistence** If a transformation is reused by different actions, it should be persisted (in-memory, file system,
or their combination). Otherwise, it will be re-calculated for each action that refers to it.
* **Shuffle operations** trigger redistributing data across the Spark cluster. Shuffles can be very expensive as they
involve I/O, serialization and network operations.

Spark programmers can benefit from several extensions. Here are a few examples:

* [SparkSQL](http://spark.apache.org/docs/latest/sql-programming-guide.html) provides a programming abstraction called
DataFrames and can also act as distributed SQL query engine.
Spark SQL can also be used to read data from an existing Hadoop Hive installation.
* [MLib](http://spark.apache.org/docs/latest/mllib-guide.html) machine learning library provides a number of statistical, classification, clustering, etc., tools that can be
used on Spark DataFrames.
* [SparkR](http://spark.apache.org/docs/latest/sparkr.html) — R on Spark** SparkR is an R package that provides a light-weight front-end to use Apache Spark from R.


In the [second post](../06/1Setup.html) of the series, I will show how Spark is installed and create a simple test application.
