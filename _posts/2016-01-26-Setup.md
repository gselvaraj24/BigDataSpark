---
layout: post
title: Spark installation, set-up and a simple test application
summary: This post describes how Spark is installed and set up. I also develop a simple test application.
---

This post continues from where we left [here](../25/Intro.html). I will describe how Spark is installed and develop a
Spark application for extracting bus running time data correlated with the local weather. In Part 3, I will first analyze
the results with R and then show how to run the application in a cluster, first locally and then on
[Amazon Elastic MapReduce](https://aws.amazon.com/elasticmapreduce/details/spark/).
The source code and more detailed instructions are provided in the [companion Github project](https://github.com/nuvostaq/BigDataSpark).

The sample project uses [Scala](http://scala-lang.org/) as its programming language. Scala is the native language of Spark,
and also well suited for parallel programming. Spark supports also Java and Python. Check out the
[Spark programming guide](http://spark.apache.org/docs/latest/programming-guide.html) for further details.

The examples have been tested on Mac OS X and Ubuntu 14, and should work as such on any *nix system. They should also work on
Windows with minimal modifications to the shell line commands.

## About the case study targets

As described in the previous post, the objective of this case study is to analyze Tampere city bus transportation
running times on specific bus lines and routes, and correlate the variations with the following parameters:

* Day classification: month and season, weekday, working day vs. weekend or holiday
* Local weather: temperature, amount of rain

The sample will contain so limited amount of data that no far-reaching conclusions can be made from it, but hopefully it
will be enough to illustrate the concepts.

## Choice of the programming language

Spark runs on the Java Virtual Machine (JVM) and is therefore compatible with the vast number of open source and commercial
libraries available in the Java ecosystem. As mentioned above, there are three great choices for programming Spark applications.

[Scala](http://www.scala-lang.org/) is the native language of Spark. Scala provides the “best of” both object oriented and functional programming, and
is very well suited for solving parallel computation problems. Scala has very advanced features, which can feel a bit
overwhelming in the beginning. However, especially Java programmers can learn Scala in baby-steps, but naturally should not
stick with Java idioms. Some rationale for the choice can be found [here](https://www.quora.com/Why-is-Apache-Spark-implemented-in-Scala).

Python is ubiquitous and also extremely popular among data scientists. Spark’s Python support is called [PySpark](https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals).
Spark’s Python documentation is [here](http://spark.apache.org/docs/latest/api/python/).

Java is also [supported](http://spark.apache.org/docs/latest/api/java/). Even with Java 8, Java programs are more verbose
than their Python and Scala counterparts. Java nevertheless provides a solid and familiar foundation for writing Spark applications.

I chose to use Scala for this project, both because I have a strong Java background, it’s the Spark team’s choice and
because learning Scala was long overdue on my personal todo list.

## Spark installation and initial project set-up

First, we need to make sure all the pre-requisites are in place. Then I will show how to build and run the initial sample
application.

### Pre-requisites

* Java 7, or greater. OpenJDK is fine.
* Scala 2.10 (used instead of the current 2.11 since it's the Amazon default) [Download](http://www.scala-lang.org/download/2.10.6.html)
* SBT (Scala build tool): [Set-up instructions](http://www.scala-sbt.org/release/tutorial/Setup.html)
* Apache Spark 1.5.2: [Download](http://spark.apache.org/downloads.html)
    * 1) Spark release 1.5.2
    * 2) Package type: pre-built for Hadoop 2.6)
* A good IDE with Scala support will help. I use IntelliJ Idea with the Scala plugin.
[Scala-IDE](http://scala-ide.org/) is another good alternative.

### Building and running the sample application

The sample can be obtained by cloning it

{% highlight bash %}
git clone https://github.com/nuvostaq/BigDataSpark.git
{% endhighlight %}

Next, check out the initial sample by running

{% highlight bash %}
cd BigDataSpark
git checkout p2-initial
{% endhighlight %}

The sample can be built by running (ensure sbt is on your path)

{% highlight bash %}
sbt assembly
{% endhighlight %}

The project uses the SBT assembly plugin to bundle the necessary dependencies into an über-JAR. However, the dependencies
are defines as "provided" whenever they are provided by Spark. More details about the project files can be found
on the [project wiki](https://github.com/nuvostaq/BigDataSpark/wiki/Project-set-up-(local)). Please check it out
also in case of build issues.

Now that we have installed all the pre-requisites and built our JAR, it's time to run the first application:

{% highlight bash %}
export MAIN=com.nuvostaq.bigdataspark.BusDataDriver
export APPJAR=target/scala-2.10/BigDataSpark-assembly-0.1.0.jar
export DATADIR=src/test/resources
$SPARKHOME/bin/spark-submit --class $MAIN $APPJAR $DATADIR/BusRoute.16784.21.5.gz $DATADIR/localweather.16786.4.0.gz $DATADIR/BusActivity.16785.8.0.gz /tmp/sparkout
{% endhighlight %}

Spark-submit<sup>[1](#footnote1)</sup> is used to run Spark application either locally, on a Spark managed cluster or on a cluster managed by YARN
or Apache Mesos. In the example above, I specify the main class of the driver program and the JAR. The rest of the parameters
 are command line parameters for our driver. In this case:

* *BusRoute* containing the raw location data of the bus routes<sup>[2](#footnote2)</sup>
* *localweather* containing the temperature and rain amount
* *BusActivity* containing the bus locations recorded at 5-second intervals
* Output directory name template, where the results will be written (e.g., /tmp/sparkout.buses/part-00000)

At the moment, the driver program looks like this.

<script src="https://gist.github.com/nuvostaq/2ab5e9250ac32bbc6e4a.js"></script>

It simply loads the input data (Spark automatically handles zipped files via Hadoop) to RDDs and saves the result.
Spark handles input on a per-file and per-line basis. The content of each gzipped file is always handled by a single
machine (gzip is a non-splittable compression format). As long as the files are not too big, this shouldn't become
an issue. If it does then some other compression scheme, e.g., [snappy](http://google.github.io/snappy/),
should be selected.
Among the various log messages<sup>[3](#footnote3)</sup>, you should be able to spot the following three lines:

{% highlight bash %}
# route entries = 2
# weather entries = 1
# bus activity entries = 115
{% endhighlight %}

The output from the final stage of the driver can be viewed like this:

{% highlight bash %}
gzcat /tmp/sparkout.dist/part-00000.gz
(16785-12-Hallila-0725,RoutePoint(12,Hallila - Keskustori P,0725,1450245589025,9330.227504218083))
{% endhighlight %}

The output contains a pair RDD<sup>[4](#footnote4)</sup>, indexed by a combination of the epoch day, line number,
start point, and bus id (scheduled start time HHMM). The value contains a RoutePoint object with partially
overlapping information, and the actual payload; the time stamp and current distance from the start for each sample.

Now we have manage to build our first application and run it. In the [next post](../27/ExtractingData.html),
I will show how Spark can be used to extract data from the bus timing and weather data sources.

### Footnotes
<div class="footnote">
<a name="footnote1">1</a>: 	Make sure that SPARKHOME either points to your Spark installation
							or that you use an absolute path in its place.
<br/>
<a name="footnote2">2</a>: Note that the file names contain the "Epoch day" (the number of calendar days since the Unix epoch) on which the
                           file was created. Weather data is read on the following day, hence the epoch day is greater by one. And since the
                           bus routes don't change that often, its epoch day is also different.
<br/>
<a name="footnote3">3</a>: 	Logging (provided by log4j) level can be set in $SPARKHOME/conf/log4j.properties.
							log4j.properties.template can be used as the starting point.
                           	'log4j.rootCategory=INFO, console' controls the log level
                           	(WARN, INFO, DEBUG are valid log levels)
<br/>
<a name="footnote4">4</a>: Pair RDDs consist of key-value pairs. Spark provides a number of useful
							<a href="https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions">
							operations on pair RDDs</a>.

</div>
